{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gpytorch, random\n",
    "import numpy as np\n",
    "from gpytorch import settings\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with input dimension of 3\n"
     ]
    }
   ],
   "source": [
    "def load_uci_data(\n",
    "    data_dir,\n",
    "    dataset,\n",
    "    seed,\n",
    "    train_pct = 0.8,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "\n",
    "    data = torch.Tensor(loadmat(data_dir + dataset + \".mat\")[\"data\"])\n",
    "\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    good_dimensions = X.var(dim=-2) > 1.0e-10\n",
    "    if int(good_dimensions.sum()) < X.size(1):\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Removed %d dimensions with no variance\"\n",
    "                % (X.size(1) - int(good_dimensions.sum()))\n",
    "            )\n",
    "        X = X[:, good_dimensions]\n",
    "\n",
    "    X_min, X_max = X.min(0)[0], X.max(0)[0]\n",
    "    X = X - X_min\n",
    "    X = 2.0 * (X / X_max) - 1.0\n",
    "    y_mu, y_std = y.mean(), y.std()\n",
    "    y -= y_mu\n",
    "    y /= y_std\n",
    "\n",
    "    shuffled_indices = torch.randperm(X.size(0))\n",
    "    X = X[shuffled_indices, :]\n",
    "    y = y[shuffled_indices]\n",
    "\n",
    "    train_n = int(floor(train_pct * X.size(0)))\n",
    "\n",
    "    train_x = X[:train_n, :].contiguous().to(device)\n",
    "    train_y = y[:train_n].contiguous().to(device)\n",
    "\n",
    "    test_x = X[train_n:, :].contiguous().to(device)\n",
    "    test_y = y[train_n:].contiguous().to(device)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loaded data with input dimension of {}\".format(test_x.size(-1)))\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, [X_min, X_max, y_mu, y_std]\n",
    "train_x, train_y, test_x, test_y, transforms = load_uci_data(\"../alternating-projection-for-gp/uci/\", \"3droad\", 0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.keops.MaternKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "model.cuda()\n",
    "likelihood.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_small, train_y_small = torch.rand([2000,10], device=\"cuda:0\"), torch.rand([2000], device=\"cuda:0\")\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "likelihood_small = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_small = ExactGPModel(train_x_small, train_y_small, likelihood_small)\n",
    "model_small.cuda()\n",
    "likelihood_small.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = likelihood(model(train_x)).lazy_covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pivoted Cholesky preconditioner\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 238.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 20.22 GiB is allocated by PyTorch, and 137.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gpytorch\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcg_tolerance(\u001b[38;5;241m0.1\u001b[39m), gpytorch\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmax_preconditioner_size(\u001b[38;5;241m150\u001b[39m), gpytorch\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39muse_pivchol_preconditioner():\n\u001b[1;32m      2\u001b[0m     lt \u001b[38;5;241m=\u001b[39m likelihood(model(train_x))\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix\n\u001b[0;32m----> 3\u001b[0m     _, plt, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py:80\u001b[0m, in \u001b[0;36mAddedDiagLazyTensor._preconditioner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39muse_pivchol_preconditioner\u001b[38;5;241m.\u001b[39mon():\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Pivoted Cholesky preconditioner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pivchol_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39muse_nyssvd_preconditioner\u001b[38;5;241m.\u001b[39mon():\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Nystrom Randomised SVD Preconditioner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py:117\u001b[0m, in \u001b[0;36mAddedDiagLazyTensor._pivchol_preconditioner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_q_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     max_iter \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_preconditioner_size\u001b[38;5;241m.\u001b[39mvalue()\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_piv_chol_self \u001b[38;5;241m=\u001b[39m \u001b[43mpivoted_cholesky\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivoted_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(torch\u001b[38;5;241m.\u001b[39misnan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_piv_chol_self))\u001b[38;5;241m.\u001b[39mitem():\n\u001b[1;32m    119\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    121\u001b[0m             NumericalWarning,\n\u001b[1;32m    122\u001b[0m         )\n",
      "File \u001b[0;32m~/gpytorch/gpytorch/utils/pivoted_cholesky.py:76\u001b[0m, in \u001b[0;36mpivoted_cholesky\u001b[0;34m(matrix, max_iter, error_tol)\u001b[0m\n\u001b[1;32m     74\u001b[0m L_m_new \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, pi_i)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     L_prev \u001b[38;5;241m=\u001b[39m L[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :m, :]\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[43mpi_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     update \u001b[38;5;241m=\u001b[39m L[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :m, :]\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, pi_m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mpi_m\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m batch_shape), m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     78\u001b[0m     L_m_new \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(update \u001b[38;5;241m*\u001b[39m L_prev, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 238.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 20.22 GiB is allocated by PyTorch, and 137.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "with gpytorch.settings.cg_tolerance(0.1), gpytorch.settings.max_preconditioner_size(150), gpytorch.settings.use_pivchol_preconditioner():\n",
    "    lt = likelihood(model(train_x)).lazy_covariance_matrix\n",
    "    _, plt, _ = lt._preconditioner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([347899, 347899])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.likelihood.noise = 0.05\n",
    "model.likelihood.noise.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gpytorch.settings.cg_tolerance(0.1), gpytorch.settings.max_preconditioner_size(0), gpytorch.settings.max_nyssvd_preconditioner_size(15):\n",
    "    gpytorch.settings.record_residual.lst_residual_norm = []\n",
    "    mll(model(train_x), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cg = gpytorch.settings.record_residual.lst_residual_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.scatter([i for i in range(len(res_cg))], torch.log(torch.tensor(res_cg)), marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some test stuff for linear solves with alt-proj and cj*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), gpytorch.settings.cg_tolerance(0.1):\n",
    "    gpytorch.settings.record_residual.lst_residual_norm = []\n",
    "    sol = gpytorch.utils.alternating_projection(train_x, model.covar_module, 0.01, train_y, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_altproj = gpytorch.settings.record_residual.lst_residual_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.lazy import DiagLazyTensor, AddedDiagLazyTensor\n",
    "with torch.no_grad(), gpytorch.settings.cg_tolerance(0.1), gpytorch.settings.max_preconditioner_size(500), gpytorch.settings.max_preconditioner_size(0):\n",
    "    gpytorch.settings.record_residual.lst_residual_norm = []\n",
    "    lt = AddedDiagLazyTensor(model.covar_module(train_x), DiagLazyTensor(torch.tensor([0.01 for i in range(len(train_y))]).cuda()))\n",
    "    precon_closure = lt._preconditioner()[0]\n",
    "    gpytorch.utils.linear_cg(lt.matmul, train_y, preconditioner=precon_closure, max_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cg = gpytorch.settings.record_residual.lst_residual_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.scatter([i for i in range(len(res_altproj))], torch.log(torch.tensor(res_altproj)), marker='.')\n",
    "plt.scatter([i for i in range(len(res_cg))], torch.log(torch.tensor(res_cg)), marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPCholesky trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, k = 1000, 1000\n",
    "A = torch.rand([N,N], device='cuda:0')\n",
    "A = 0.1 * A.T @ A + 0.01 * torch.eye(N, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = A.diag()\n",
    "\n",
    "##G is the skinny factor transposed\n",
    "G = torch.zeros([k,N], device='cuda:0')\n",
    "arr_idx = []\n",
    "\n",
    "for i in range(k):\n",
    "    idx = torch.multinomial(diags/diags.sum(), 1)\n",
    "    arr_idx.append(idx)\n",
    "    G[i,:] = (A[idx,:] - G[:i,idx].T @ G[:i,:]) / torch.sqrt(diags[idx])\n",
    "    diags -= G[i,:]**2\n",
    "    diags = diags.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt._lazy_tensor._approx_diag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
